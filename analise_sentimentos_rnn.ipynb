{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:04.342503Z",
     "start_time": "2024-09-21T03:18:01.560784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzogiordanoaraujo/cursosAlura/RNN_curso/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from jinja2 import optimizer\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from processamento.preparar_texttorch import tokenize, construir_vocabulario, tokens_to_ids, pad_sequences, preparar_dataloader, dividir_treino_validacao"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_training = pd.read_csv('twitter_training.csv')\n",
    "df_training = df_training.dropna(subset=[df_training.columns[2], df_training.columns[3]])\n",
    "dados_treinamento = [\n",
    "    (row[df_training.columns[2]], tokenize(row[df_training.columns[3]]))\n",
    "    for _, row in df_training.iterrows()\n",
    "    if isinstance(row[df_training.columns[3]], str)\n",
    "]\n",
    "\n",
    "df_test = pd.read_csv('twitter_validation.csv')\n",
    "df_test = df_test.dropna(subset=[df_test.columns[2], df_test.columns[3]])\n",
    "dados_teste = [\n",
    "    (row[df_test.columns[2]], tokenize(row[df_test.columns[3]]))\n",
    "    for _, row in df_test.iterrows()\n",
    "    if isinstance(row[df_test.columns[3]], str)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:06.129027Z",
     "start_time": "2024-09-21T03:18:04.344993Z"
    }
   },
   "id": "96f3a1fb6ebb50b9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Positive', ['i', 'am', 'coming', 'to', 'the', 'borders', 'and', 'i', 'will', 'kill', 'you', 'all,'])\n"
     ]
    }
   ],
   "source": [
    "print(dados_treinamento[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:06.135523Z",
     "start_time": "2024-09-21T03:18:06.129060Z"
    }
   },
   "id": "c7e9eece506be373",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 71147\n",
      "Exemplo de vocabulário: [('i', 0), ('am', 1), ('coming', 2), ('to', 3), ('the', 4), ('borders', 5), ('and', 6), ('will', 7), ('kill', 8), ('you', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocabulario = construir_vocabulario(dados_treinamento)\n",
    "print(f\"Tamanho do vocabulário: {len(vocabulario)}\")\n",
    "print(f\"Exemplo de vocabulário: {list(vocabulario.items())[:10]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:06.228905Z",
     "start_time": "2024-09-21T03:18:06.186682Z"
    }
   },
   "id": "ac199d8f595f9603",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [0, 1, 2, 3, 4, 5, 6, 0, 7, 8, 9, 10])\n"
     ]
    }
   ],
   "source": [
    "dados_treinamento_ids = tokens_to_ids(dados_treinamento, vocabulario)\n",
    "dados_teste_ids = tokens_to_ids(dados_teste, vocabulario)\n",
    "\n",
    "print(dados_treinamento_ids[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:06.468174Z",
     "start_time": "2024-09-21T03:18:06.372904Z"
    }
   },
   "id": "d9dc23dbac499728",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [0, 1, 2, 3, 4, 5, 6, 0, 7, 8, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "max_len = 64\n",
    "dados_treinamento_padded = pad_sequences(dados_treinamento_ids, max_len)\n",
    "dados_teste_padded = pad_sequences(dados_teste_ids, max_len)\n",
    "\n",
    "print(dados_treinamento_padded[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:06.650679Z",
     "start_time": "2024-09-21T03:18:06.644756Z"
    }
   },
   "id": "fb46d82b3225d283",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 799, Validação: 200\n"
     ]
    }
   ],
   "source": [
    "dados_treinamento_final, dados_validacao = dividir_treino_validacao(dados_teste_padded)\n",
    "\n",
    "print(f\"Treino: {len(dados_treinamento_final)}, Validação: {len(dados_validacao)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:06.651202Z",
     "start_time": "2024-09-21T03:18:06.647327Z"
    }
   },
   "id": "8d0780c51dc4170e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  120,   356,   115,  ...,     0,     0,     0],\n",
      "        [ 8565,   782, 10113,  ...,     0,     0,     0],\n",
      "        [    0,   464,   108,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   40,  4358,   846,  ...,     0,     0,     0],\n",
      "        [  196,   643,    38,  ...,     0,     0,     0],\n",
      "        [ 6272, 14296, 16274,  ...,     0,     0,     0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_loader = preparar_dataloader(dados_treinamento_final, batch_size, drop_last=True)\n",
    "valid_loader = preparar_dataloader(dados_validacao, batch_size, drop_last=True)\n",
    "test_loader = preparar_dataloader(dados_teste_padded, batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:06.683442Z",
     "start_time": "2024-09-21T03:18:06.653112Z"
    }
   },
   "id": "730accfe4adccac4",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(71147, 100)\n",
      "  (rnn): GRU(100, 256)\n",
      "  (linear): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tam_vocab, tam_embedding, embed_vectors, \n",
    "                   ind_unk, ind_pad, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "    \n",
    "        # Inicializaremos a camada de embedding\n",
    "        self.embedding = nn.Embedding(tam_vocab, tam_embedding)\n",
    "        self.embedding.weight.data.copy_(embed_vectors)\n",
    "        self.embedding.weight.data[ind_unk] = torch.zeros(tam_embedding)\n",
    "        self.embedding.weight.data[ind_pad] = torch.zeros(tam_embedding)\n",
    "        #######################################\n",
    "    \n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(tam_embedding, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, X, tamanhos):\n",
    "        embed = self.embedding(X)\n",
    "        \n",
    "        # Inicializar a memória\n",
    "        hidden = torch.zeros(1, X.size(1), self.hidden_size)\n",
    "        \n",
    "        # Empacotar a sequencia antes de alimentar a uniadde recorrente\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embed, tamanhos)\n",
    "        \n",
    "        # Forward recorrente\n",
    "        packed_output, hidden = self.rnn(packed_input, hidden)\n",
    "        \n",
    "        # Desempacotar a sequência para continuar o fluxo na rede\n",
    "        output, output_lenghts = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        output = self.linear(hidden.squeeze())\n",
    "        \n",
    "        return output\n",
    "        \n",
    "tam_vocab = len(vocabulario)\n",
    "tam_embedding = 100  # tamanho do embedding\n",
    "embed_vectors = torch.rand(tam_vocab, tam_embedding)  # inicialização\n",
    "ind_unk = vocabulario.get('<unk>', 0)  # Índice para <unk>\n",
    "ind_pad = vocabulario.get('<pad>', 0)  # Índice para <pad>\n",
    "hidden_size = 256  # tamanho do estado oculto (Neurônios ocultos)\n",
    "output_size = 3  # Exemplo de número de classes\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNN(tam_vocab, tam_embedding, embed_vectors, ind_unk, ind_pad, hidden_size, output_size)\n",
    "model.to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:06.806512Z",
     "start_time": "2024-09-21T03:18:06.689384Z"
    }
   },
   "id": "6a390420367b8ba5",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "criterio = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:07.321475Z",
     "start_time": "2024-09-21T03:18:06.803599Z"
    }
   },
   "id": "7c7347809f71c64e",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def forward(iterator, num_amostra, etapa):\n",
    "    if etapa == 'treino': model.train()\n",
    "    else: model.eval()\n",
    "    \n",
    "    acuracia = 0.\n",
    "    loss_epoca = []\n",
    "    for k, amostra in enumerate(iterator):\n",
    "        texto, rotulo = amostra  # Descompactar diretamente\n",
    "        tamanhos = [len(t) for t in texto]  # Exemplo para obter tamanhos das sequências\n",
    "        \n",
    "        saida = model(texto, tamanhos)\n",
    "        \n",
    "        loss = criterio(saida, rotulo)\n",
    "        loss_epoca.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        _, pred = torch.max(saida, axis=-1)\n",
    "        acuracia += (pred.cpu().data == rotulo.cpu().data).sum()\n",
    "        \n",
    "        if etapa == 'treino':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    loss_epoca = np.asarray(loss_epoca).ravel()\n",
    "    acuracia = acuracia / float(num_amostra)\n",
    "    print('\\n', '*'*15 + etapa + '*'*15)\n",
    "    print('Epoca: {:}, Loss: {:.4f} +/- {:.4f}, Acurácia: {:.4f}'.format(\n",
    "        epoca, loss_epoca.mean(), loss_epoca.std(), acuracia\n",
    "    ))\n",
    "    \n",
    "    return loss_epoca.mean(), acuracia\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:07.327427Z",
     "start_time": "2024-09-21T03:18:07.323791Z"
    }
   },
   "id": "56e72ad3e5af59dc",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ***************treino***************\n",
      "Epoca: 0, Loss: 0.9823 +/- 0.0676, Acurácia: 0.7447\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 0, Loss: 0.8670 +/- 0.0922, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 1, Loss: 0.7776 +/- 0.0996, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 1, Loss: 0.8051 +/- 0.0892, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 2, Loss: 0.6478 +/- 0.1031, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 2, Loss: 0.6302 +/- 0.0766, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 3, Loss: 0.4696 +/- 0.0884, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 3, Loss: 0.3639 +/- 0.0364, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 4, Loss: 0.2803 +/- 0.0712, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 4, Loss: 0.1486 +/- 0.0305, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 5, Loss: 0.0495 +/- 0.0600, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 5, Loss: 0.0011 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 6, Loss: 0.0007 +/- 0.0002, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 6, Loss: 0.0004 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 7, Loss: 0.0004 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 7, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 8, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 8, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 9, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 9, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 10, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 10, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 11, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 11, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 12, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 12, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 13, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 13, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 14, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 14, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 15, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 15, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 16, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 16, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 17, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 17, Loss: 0.0003 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 18, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 18, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 19, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 19, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 20, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 20, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 21, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 21, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 22, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 22, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 23, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 23, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9600\n",
      "\n",
      " ***************treino***************\n",
      "Epoca: 24, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9612\n",
      "\n",
      " ***************teste***************\n",
      "Epoca: 24, Loss: 0.0002 +/- 0.0000, Acurácia: 0.9600\n"
     ]
    }
   ],
   "source": [
    "loss_treino, loss_test = [], []\n",
    "acc_treino, acc_test = [], []\n",
    "\n",
    "for epoca in range(25):\n",
    "    loss, acuracia = forward(train_loader, len(dados_treinamento_final), 'treino')\n",
    "    loss_treino.append(loss)\n",
    "    acc_treino.append(acuracia)\n",
    "    \n",
    "    loss, acuracia = forward(valid_loader, len(dados_validacao), 'teste')\n",
    "    loss_test.append(loss)\n",
    "    acc_test.append(acuracia)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:57.832126Z",
     "start_time": "2024-09-21T03:18:07.327219Z"
    }
   },
   "id": "fc872cb9c2a5dbf9",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men_core_web_sm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Certifique-se de ter esse modelo instalado\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_sentiment\u001B[39m(sentence):\n\u001B[1;32m      4\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m~/cursosAlura/RNN_curso/.venv/lib/python3.9/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/cursosAlura/RNN_curso/.venv/lib/python3.9/site-packages/spacy/util.py:472\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[0;32m--> 472\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[0;31mOSError\u001B[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  # Certifique-se de ter esse modelo instalado\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenizar a frase usando spaCy\n",
    "    tokenized = [str(tok) for tok in nlp(sentence)]\n",
    "    print(\"Tokenized:\", tokenized)\n",
    "\n",
    "    # Converter tokens em IDs usando o vocabulário\n",
    "    indexed = [vocabulario.get(t, vocabulario['<unk>']) for t in tokenized]  # Usar <unk> para palavras desconhecidas\n",
    "    if len(indexed) == 0:\n",
    "        raise ValueError(\"A frase não contém palavras do vocabulário.\")\n",
    "    \n",
    "    # Comprimento da sequência\n",
    "    length = [len(indexed)]\n",
    "    print(\"Indexed:\", indexed)\n",
    "\n",
    "    # Criar tensor e adicionar dimensão para batch\n",
    "    tensor = torch.LongTensor(indexed).unsqueeze(0).to(device)  # Adicionando dimensão para batch_size = 1\n",
    "    length_tensor = torch.LongTensor(length).to(device)\n",
    "\n",
    "    # Fazer a predição\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tensor, length_tensor)\n",
    "\n",
    "    return F.softmax(prediction, dim=-1).cpu().numpy()  # Retornar como numpy\n",
    "\n",
    "# Testar a função com frases de exemplo\n",
    "exemplos = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"It's okay, not great but not bad either.\",\n",
    "    \"Absolutely fantastic service!\",\n",
    "    \"I'm very disappointed with the quality.\"\n",
    "]\n",
    "\n",
    "# Gerar previsões e plotar os resultados\n",
    "for frase in exemplos:\n",
    "    pred = predict_sentiment(frase)\n",
    "\n",
    "    # Plotar os resultados\n",
    "    plt.bar(0, pred[0], color='darkred', label='Negativo', width=0.5)\n",
    "    plt.bar(1, pred[1], color='dodgerblue', label='Positivo', width=0.5)\n",
    "    plt.title(f\"Sentiment Prediction for: '{frase}'\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T03:18:58.143309Z",
     "start_time": "2024-09-21T03:18:57.834454Z"
    }
   },
   "id": "34663275c1df8003",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-21T03:18:58.140269Z"
    }
   },
   "id": "7e54e9ee775d88bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = preparar_dataloader(dados_treinamento_final, batch_size)\n",
    "valid_loader = preparar_dataloader(dados_validacao, batch_size)\n",
    "test_loader = preparar_dataloader(dados_teste_padded, batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdb6645f70cd372e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c270488255f30d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
